# ── AI Provider Selection ──────────────────────────────────────────────────
# Choose which AI backend to use:
#   "bedrock" | "anthropic" | "openai" | "azure-openai" | "github-models" |
#   "openrouter" | "groq" | "gemini" | "deepseek" | "ollama"
AI_PROVIDER=bedrock

# ── AWS Bedrock (when AI_PROVIDER=bedrock) ─────────────────────────────────
# AWS_ACCESS_KEY_ID=your-aws-access-key-here
# AWS_SECRET_ACCESS_KEY=your-aws-secret-key-here
# AWS_REGION=us-east-1

# ── Anthropic Direct (when AI_PROVIDER=anthropic) ──────────────────────────
# ANTHROPIC_API_KEY=sk-ant-api03-...
# ANTHROPIC_MODEL=claude-sonnet-4-20250514
# ANTHROPIC_API_ENDPOINT=https://api.anthropic.com/v1/messages

# ── OpenAI (when AI_PROVIDER=openai) ───────────────────────────────────────
# OPENAI_API_KEY=sk-proj-...
# OPENAI_MODEL=gpt-4o
# OPENAI_API_ENDPOINT=https://api.openai.com/v1/chat/completions

# ── Azure OpenAI (when AI_PROVIDER=azure-openai) ──────────────────────────
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_API_KEY=your-azure-key
# AZURE_OPENAI_DEPLOYMENT=gpt-4o
# AZURE_OPENAI_API_VERSION=2024-10-21

# ── GitHub Models (when AI_PROVIDER=github-models) ─────────────────────────
# Uses the same GITHUB_TOKEN below, or set a separate one:
# GITHUB_MODELS_TOKEN=ghp_your-github-pat-with-models-scope
# GITHUB_MODELS_ENDPOINT=https://models.inference.ai.azure.com/chat/completions

# ── OpenRouter (when AI_PROVIDER=openrouter) ───────────────────────────────
# Unified gateway to 200+ models — https://openrouter.ai
# OPENROUTER_API_KEY=sk-or-v1-...
# OPENROUTER_MODEL=anthropic/claude-sonnet-4

# ── Groq (when AI_PROVIDER=groq) ──────────────────────────────────────────
# Ultra-fast inference — https://console.groq.com
# GROQ_API_KEY=gsk_...
# GROQ_MODEL=llama-3.3-70b-versatile

# ── Google Gemini (when AI_PROVIDER=gemini) ────────────────────────────────
# GOOGLE_API_KEY=AIza...
# GEMINI_MODEL=gemini-2.5-flash

# ── DeepSeek (when AI_PROVIDER=deepseek) ───────────────────────────────────
# DEEPSEEK_API_KEY=sk-...
# DEEPSEEK_MODEL=deepseek-chat

# ── Ollama / Local (when AI_PROVIDER=ollama) ──────────────────────────────
# Run models locally — https://ollama.ai
# OLLAMA_ENDPOINT=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.2

# ── Model Overrides (per-task or global) ───────────────────────────────
# Override the model for ALL tasks at once:
# AI_MODEL=gpt-4o-mini
#
# Or override per task (takes priority over AI_MODEL):
# AI_MODEL_CLASSIFIER=gpt-4o-mini
# AI_MODEL_COMMENT=gpt-4o-mini
# AI_MODEL_DUPLICATE=gpt-4o-mini
#
# Custom cost rates (USD per 1K tokens) — overrides the built-in table:
# AI_COST_PER_1K_INPUT=0.005
# AI_COST_PER_1K_OUTPUT=0.015
#
# Or override the cost rate multiplier (1.0 = Claude Sonnet 4 baseline):
# AI_COST_RATE=2.2

# ── AI Proxy Backend ───────────────────────────────────────────────────────
# Port for the self-hosted proxy server (default: 3456)
# AI_PROXY_PORT=3456
#
# Optional API key to protect the proxy (leave empty for open access)
# AI_PROXY_API_KEY=my-secret-key

# ── GitHub Token (Required for issue operations) ───────────────────────────
GITHUB_TOKEN=your-github-token-here

# Repository Info (Optional - for duplicate detection testing)
REPOSITORY_OWNER=kirodotdev
REPOSITORY_NAME=kiro

# Test Issue Data (Optional - for full triage testing)
ISSUE_NUMBER=123
ISSUE_TITLE=Test issue title
ISSUE_BODY=Test issue description

# Notes:
# 1. Copy this file to .env: cp .env.example .env
# 2. Fill in your actual credentials
# 3. Load variables: export $(cat .env | xargs)
# 4. Run tests: npm run test:local
# 5. NEVER commit .env file to git!
